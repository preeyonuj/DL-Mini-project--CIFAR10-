{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchsummary\n",
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "#random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations to be added to the train dataset\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "])\n",
    "\n",
    "\n",
    "# Transformations to be added to the test dataset\n",
    "# Test transformations \n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Train and valid Loader ######################\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform)\n",
    "\n",
    "# Validation Data ratio from Train data\n",
    "VALID_RATIO = 0.8\n",
    "# Number of train samples\n",
    "n_train_examples = int(len(trainset) * VALID_RATIO)\n",
    "# Number of test samples\n",
    "n_valid_examples = len(trainset) - n_train_examples\n",
    "\n",
    "# Dividing train and validation based on number of samples\n",
    "train_data, valid_data = data.random_split(trainset, \n",
    "                                           [n_train_examples, n_valid_examples])\n",
    "\n",
    "# Train Data Loader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# Applying test transform on validation data after a deepcopy\n",
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transform = test_transform\n",
    "\n",
    "# Validation Data Loader\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    valid_data, batch_size=1024, shuffle=False, num_workers=2)\n",
    "\n",
    "###################### Test Loader ######################\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=500, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "###################### Datalaoders ######################\n",
    "dataloaders = {'train': trainloader,  'valid': validloader, 'test': testloader}\n",
    "dataset_sizes = {'train': len(trainloader.dataset), 'valid': len(validloader.dataset), 'test': len(testloader.dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names in the CIFAR10 dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # Conv followed by BN\n",
    "        # Since image size is (32, 32), kernel size is kept small with limited padding\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "         # second batch of conv layers followed by BN\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # Sequential layer to represent the skip connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, padding=0, stride=stride, bias=True),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Tried out - ReLU and Leaky Relu where results were pretty similar\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # Here is the skip connection\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 32\n",
    "        \n",
    "        # First conv layer followed by BN\n",
    "        # Size has been kept at 32, to facillitate higher number of layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Make 4 batches of layers with different input size\n",
    "        # Have kept the block sizes power of 2s\n",
    "        # Added a few dropout layers for the \n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.drop_out_1 = nn.Dropout(p=0.5)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.drop_out_2 = nn.Dropout(p=0.5)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        # Sequential layer to map final block size to number of classes\n",
    "        self.linear = nn.Linear(256*block.expansion, num_classes)\n",
    "\n",
    "    # Building basic blocks in a modular fashion\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We experimented with the order of dropout within these layers\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop_out_1(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = self.drop_out_2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 4758026\n"
     ]
    }
   ],
   "source": [
    "# Config for different sized blocks\n",
    "config = [4, 4, 4, 3]\n",
    "\n",
    "# Model Creation\n",
    "model = ResNet(BasicBlock, config)\n",
    "\n",
    "print(\"Number of trainable parameters:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaiming initialization for Conv Layer and general initialization for BN and Linear\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity = 'relu')\n",
    "        nn.init.constant(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant(m.weight.data, 1)\n",
    "        nn.init.constant(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal(m.weight.data, std=1e-3)\n",
    "        nn.init.constant(m.bias.data, 0)\n",
    "\n",
    "# Applying Initializations\n",
    "model.apply(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting which device to train the model on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model onto device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# For multiclass image classification, crossentropy worked great compared to multimargin loss\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Epochs\n",
    "EPOCH = 100\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# weight decay\n",
    "weightDecay = 0.0001\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weightDecay)\n",
    "\n",
    "# Schedulers\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor = 0.5, patience=5, cooldown=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    start = time.time()\n",
    "    # Storing loss and accuracy to be used later to plot\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_acc, valid_acc = [], []\n",
    "\n",
    "    # Copying the initial network weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step(running_loss)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            \n",
    "            # Storing every train and validation epoch accuracy and loss\n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "            elif phase == 'valid':\n",
    "                valid_losses.append(epoch_loss)\n",
    "                valid_acc.append(epoch_acc)            \n",
    "            \n",
    "        \n",
    "    time_elapsed = time.time() - start\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "\n",
    "    # load best model weightsssss\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we are training the model 3 times sequentially to manually simulate a restart\n",
    "first_run_train_losses, first_run_valid_losses, first_run_train_acc, first_run_valid_acc, model = train_model(model, \n",
    "                                                                                                     criterion, \n",
    "                                                                                                     optimizer, \n",
    "                                                                                                     scheduler, \n",
    "                                                                                                     EPOCH)\n",
    "\n",
    "PATH = \"training_3_1_f.pt\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# 1st Restart\n",
    "second_run_train_losses, second_run_valid_losses, second_run_train_acc, second_run_valid_acc, model = train_model(model, \n",
    "                                                                                                     criterion, \n",
    "                                                                                                     optimizer, \n",
    "                                                                                                     scheduler, \n",
    "                                                                                                     EPOCH)\n",
    "\n",
    "PATH = \"training_3_2_f.pt\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# 2nd Restart\n",
    "lr = optimizer.param_groups[0]['lr']\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weightDecay)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor = 0.5, patience=5, cooldown=2)\n",
    "third_run_train_losses, third_run_valid_losses, third_run_train_acc, third_run_valid_acc, model = train_model(model, \n",
    "                                                                                                     criterion, \n",
    "                                                                                                     optimizer, \n",
    "                                                                                                     scheduler, \n",
    "                                                                                                     EPOCH)\n",
    "\n",
    "PATH = \"training_3_3_f.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending all the train losses in a single list\n",
    "train_losses = first_run_train_losses + second_run_train_losses + third_run_train_losses\n",
    "# Appending all the validation losses in a single list\n",
    "valid_losses = first_run_valid_losses + second_run_valid_losses + third_run_valid_losses\n",
    "\n",
    "# Appending all the train accuracy in a single list\n",
    "train_acc = first_run_train_acc + second_run_train_acc + third_run_train_acc\n",
    "# Appending all the validation accuracy in a single list\n",
    "valid_acc = first_run_valid_acc + second_run_valid_acc + third_run_valid_acc\n",
    "\n",
    "# COnverting from tensor to list\n",
    "train_acc = [i.cpu().tolist() for i in train_acc]\n",
    "valid_acc = [i.cpu().tolist() for i in valid_acc]\n",
    "\n",
    "# Creating a pandas dataframe for better processing\n",
    "results_df = pd.DataFrame({'train_loss':train_losses, 'valid_loss':valid_losses, \n",
    "              'train_acc':train_acc, 'valid_acc':valid_acc})\n",
    "\n",
    "# Plotting the Loss for train and validation\n",
    "plt.figure(figsize=(10, 10))\n",
    "df[['train_loss', 'valid_loss']].plot()\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train VS Validation Loss\")\n",
    "plt.savefig(\"Loss_plot.jpg\", dpi = 160)\n",
    "\n",
    "# Plotting the accuracy for train and validation\n",
    "plt.figure(figsize=(10, 10))\n",
    "df[['train_acc', 'valid_acc']].plot()\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train VS Validation Accuracy\")\n",
    "plt.savefig(\"Acc_plot.jpg\", dpi = 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate performance metrics of results\n",
    "def performance_metrics(labels, preds, classes):\n",
    "    print(\"Overall metrics\")\n",
    "    print(\"Accuracy:\", accuracy_score(labels, preds))\n",
    "    print(\"Recall:\", recall_score(labels, preds, average='weighted'))\n",
    "    print(\"Precision:\", precision_score(labels, preds, average='weighted'))\n",
    "    print(\"F1:\", f1_score(labels, preds, average='weighted'))\n",
    "    \n",
    "    print(\"\\nIndividual Metrics\\n\")\n",
    "    recall_scores = recall_score(labels, preds, average=None)\n",
    "    precision_scores = precision_score(labels, preds, average=None)\n",
    "    f1_scores = f1_score(labels, preds, average=None)\n",
    "    return pd.DataFrame({'Class': classes, 'Recall': recall_scores, 'Precision':precision_scores,\n",
    "                 'F1':f1_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    \n",
    "    # Save last state of training before changin to eval mode\n",
    "    # This is done to resume training after eval, in case\n",
    "    was_training = model.training\n",
    "    # Eval mode of the model. So, it doesn't learn the  discrepencies\n",
    "    model.eval()\n",
    "    \n",
    "    # Empty list to hold values\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    \n",
    "    # Generates the outputs and stores it in the list\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Gernerating predictions\n",
    "            outputs = model(inputs)\n",
    "            # Acquiring predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "    \n",
    "    # Model returned to training state\n",
    "    model.train(mode=was_training)\n",
    "    return all_labels, all_preds\n",
    "    \n",
    "labels, preds = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to better formats for calculating performance metrics\n",
    "labels = [j  for i in labels for j in i.cpu().tolist()[:]]\n",
    "preds = [j  for i in preds for j in i.cpu().tolist()[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall metrics\n",
      "Accuracy: 0.9365\n",
      "Recall: 0.9365\n",
      "Precision: 0.936397965853499\n",
      "F1: 0.9364173162851157\n",
      "\n",
      "Individual Metrics\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plane</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.933663</td>\n",
       "      <td>0.938308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>car</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.957677</td>\n",
       "      <td>0.965278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bird</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.918347</td>\n",
       "      <td>0.914659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.878074</td>\n",
       "      <td>0.867409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deer</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.933860</td>\n",
       "      <td>0.939891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.904478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>frog</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.956871</td>\n",
       "      <td>0.955433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>horse</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.959799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ship</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.955912</td>\n",
       "      <td>0.954955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>truck</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.964930</td>\n",
       "      <td>0.963964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Recall  Precision        F1\n",
       "0  plane   0.943   0.933663  0.938308\n",
       "1    car   0.973   0.957677  0.965278\n",
       "2   bird   0.911   0.918347  0.914659\n",
       "3    cat   0.857   0.878074  0.867409\n",
       "4   deer   0.946   0.933860  0.939891\n",
       "5    dog   0.909   0.900000  0.904478\n",
       "6   frog   0.954   0.956871  0.955433\n",
       "7  horse   0.955   0.964646  0.959799\n",
       "8   ship   0.954   0.955912  0.954955\n",
       "9  truck   0.963   0.964930  0.963964"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results\n",
    "performance_metrics(labels, preds, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the overall accuracy is 93.65%. We also see that the cat and dog classes didn't perform well based on their Recall, Precision and F1 Score. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
